---
title: "title"
subtitle: "subtitle"
author: 
  - Michaela Drouillard
thanks: "Code and data are available at: https://github.com/michaeladrouillard/jack."
date: today
abstract: "Using data from the Spotify API, I explored Jack Antonoff's production style and assessed whether Spotify's audio features can capture it. In an interview with Glen McDonald, an engineer at Spotify, I learned that the danceability and valence features were created by having interns tag songs as positive or gloomy, or danceable or un-danceable. Using a logistic regression model, I found that danceability was the most important feature in predicting Antonoff's involvement in a song. In the discussion section, I do a close-listen of Antonoff's most danceable songs, and argue that the danceability variable is actually capturing something more like a visceral quality to Antonoff's music."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| echo: false
#| warning: false

library(tidyverse)
library(lubridate)

```

# Introduction

Shortly after the release of Taylor Swift's *Midnights* album, a video of the Youtuber Caleb Gammen, in which guesses which songs on Midnights were produced by Antonoff within seconds [@citeTweet], went viral. The tweet reads: "im able to instantly detect if jack antonoff worked on a song due to a visceral hatred of his production style". He nails it. Only one song on the album wasn't produced by Antonoff, and he guesses it.

Antonoff began his career as a guitarist in fun., and is the frontman in Bleachers. He has produced and co-produced music for Lorde, Taylor Swift, Florence and the Machine, The Chicks, Clairo, The 1975, Grimes, Zayn, Pink, Lana Del Rey, Olivia Rodrigo, the Minions: The Rise of Gru Soundtrack, and more. Antonoff is prolific, to the extent that it is starting to bother people. As Andrew Marantz writes in the New Yorker: "When his band releases an album, the world responds politely. When he produces one by Lorde or Lana Del Rey or Taylor Swift, the world wobbles on its axis" [@citeMarantz].

(Cite a few snippets from critics articles to give a dose of the discourse)

Can Spotify's audio features dataset help us determine what makes Antonoff music so Antonoff-y?

I spoke to Caleb Gamman, a Youtuber and Antonoff-production-style hater who had a tweet go viral around the time of Taylor Swift's *Midnights* release, in which he predicts whether a song was produced by Antonoff within seconds.

During an interview with Glen McDonald, an engineer at Spotify, I learned that the `valence` and `danceability` variables were created by hiring college interns and having them tag whether songs were positive or danceable.

I used two models to investigate the data. The first, a logistic regression, draws on the dataset of artists who've collaborated with Antonoff, with an extra binary variable to tag whether or not the song was an Antonoff production. With this model, we can get a first glimpse at variables that are stronger predictors of Antonoff-iness, along with a better understanding of whether the Spotify data can trace Antonoff's influence on an artist's sound over time.

However, logistic regression assumes a linear relationship between the dependent and predictor variables, and is sensitive to multi-collinearity. In music data, we would expect for certain variables to be correlated (for instance, loudness and energy). We also cannot assume that an artists' style can be measured in the linear growth of these metrics. Although the logistic regression has a strong accuracy, indicating that danceability and loudness are the strongest indicators of an Anotonoff produced song, it may be detecting a change in genre, rather than a change in style, across artists whose sound turns more into pop while collaborating with Antonoff.

I built a second model, a Random Forest, to complement the insights drawn from the logistic regression. The Random Forest trains on the full data set, including other producers. Random Forests (what's the package) don't assume linear relationships, and aren't sensitive to multicollinearity. In fact, using the x package in R, we can visualize and analyse the relationships between different variables. The variance in correlation scores across producers can be taken as an approximate for style, since this variance helps the model classify tracks.

In the discussion section, I listen to the songs with the highest danceability scores, and make an argument that this `danceability` feature, which has so much uncertainty cooked into it, is really capturing something like visceral-ness. Against the odds, we can capture something about Antonoff's music with these variables, and it might just be that subjective "ick" that some people feel, and others enjoy, and that we're all just trying to describe.

# Lit Review 

# Data {#sec-data}

The audio feature data was pulled using Spotify API, which I accessed using the `spotifyr` package in R [@citeSpotify]. I used the `get_artist_audio_features` function to acquire data on Lorde, Taylor Swift, St. Vincent, Lana Del Rey, The Chicks, Florence and the Machine, and Bleachers discographies. I threw in HAIM, Marina and the Diamonds, Maggie Rogers, Sharon Von Etten, and Mitski in too, since they've never collaborated with Jack Antonoff but are more or less part of the same pop sphere.

I also scraped the discographies of comparable contemporary pop producers' Wikipedias. The producers include Epworth, Rechstaid, Max Martin, Nowels, Joel Little, and Greg Kustin. (cite Carl here?) These producers were chosen based on their popularity, and also because some of them collaborated with artists who also collaborate with Antonoff.

The data was manually validated, and live performances, karaoke editions, international versions or translations, and remix albums were removed. Where deluxe albums were available, original albums were deleted to avoid duplicate rows.

### Artist Selection

Of the artists who did collaborate with Antonoff, I chose these artists because they've produced multiple albums, and at least one album with Antonoff, as opposed to just individual songs.

### Data Cleaning

I joined these data sets using the `rbind` function in base R, and created an additional variable called "jack", which contains a 1 if Jack Antonoff produced or co-produced the song, and a 0 if he had nothing to do with it. I got the information on which songs Antonoff did or did not produce from the "Jack Antonoff Production Discography" Wikipedia page [@citewiki].

### Audio Features Dataset

The `get_artist_audio_features` provides a profile of each song, according to variables developped by Spotify, which makes for simple comparisons across artists. The final audio features in this study data set contains the variables: `artist_name`, `track_name`, `energy`, `danceability`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `jack`.

From the `get_artist_audio_features` documentation (@citeSpotifyDocumentation):

> -   **acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
>
> -   **danceability**: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
>
> -   **energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
>
> -   **instrumentalness**: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
>
> -   **key**: The key the track is in. Integers map to pitches using standard [Pitch Class notation](https://en.wikipedia.org/wiki/Pitch_class). E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.
>
> -   **liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.
>
> -   **loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.
>
> -   **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.
>
> -   **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
>
> -   **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

It's hard to gauge exactly what the variables "danceability" or "valence" mean, even using the documentation. For instance, "valence" is described as "A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)". "Danceability" is described as "how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable". Neither of these definitions fully cover what metrics and thresholds go into something like "danceability". However, whatever these features capture is easier to compare across artists in visualizations than more granular details about bars, beats, duration, and tatums[^1].

[^1]: \-- a word I just learned, which refers to "the lowest regular pulse train that a listener intuitively infers from the timing of perceived musical events".

This is pretty vague. Especially valence -- is the positivity related to lyrics, or something else? I compared the valence scores of classical music tracks to see if this score is based on lyrics, or the just the instruments. In @tbl-Holst, Gustav Holst's Jupiter has a valence score 326.78% higher than Mars. This tracks. It suggests the valence is based on audio features other than lyrical content. It also suggests that it is, in fact, capturing a difference between the two songs' profiles.

```{r}
#| echo: false
#| warning: false
#| label: tbl-Holst
#| tbl-cap: Valence and Danceability Scores for Holst's Mars and Jupiter

sub_planets <- read_csv(here::here("inputs/data/subset_planets.csv"))

sub_planets |>
  select(artist_name, valence, danceability, track_name) |>
  head() |>
  knitr::kable(col.names = c("Artist Name",
                             "Valence",
                             "Danceability",
                             "Track Name"))
```

I looked at the valence measures on Nikolai Rimsky-Korsakov's Flight of the Bumblebee, a song that's much faster and frantic than Mars and Jupiter. What I found complicated my understanding of valence -- different performances of the same song had different valence scores (@tbl-bees). Some were arias, played by different instruments. Arias tended to have higher valence scores. The lowest recorded valence across all performances is 0.2410, and the highest is .9230. What gives?

```{r}
#| echo: false
#| warning: false
#| label: tbl-bees
#| tbl-cap: Valence and Danceability Scores for Every Performance of Flight of the Bumblebee on Spotify

sub_bees <- read_csv(here::here("inputs/data/subset_bumblebee.csv"))

sub_bees |>
  select(artist_name,
         valence,
         danceability,
         album_release_year,
         track_name) |>
  head(17) |>
  knitr::kable(
    col.names = c(
      "Artist Name",
      "Valence",
      "Danceability",
      "Album Release Year",
      "Track Name"
    )
  )
```

## Interview with Glen McDonald

I interviewed Glen McDonald, a Principal Engineer at Spotify who worked at The Echo Nest, a music intelligence start-up that was acquired by Spotify in 2014. Macdonald told me that `valence` and `danceability` were created by giving thousands of examples to college interns and asking them to tag whether a song was positive or gloomy, or danceable or un-danceable. Variables like energy or instrumentalness, on the other hand, were determined primarily through machine listening techniques. The human subjectivity part only came in when it came to tweaking the training data[^2]:

[^2]: My favourite example Glen gave was that the ML kept giving bluegrass songs insanely high `speechiness` ratings. Because there were no banjos in the training data, they were registered as human speech. (This is hilarious.) They had to go back and add more songs with banjos to the training data.

"You could imagine writing a formula for energy that combines like loudness and tempo and degree of harmonic variation or something. So it was \[machine learning\], but that one's more like a human helping a machine figure out a formula. Whereas valence is teaching the machine to try to reproduce a purely human thing. Same with danceability. I mean, danceability is whether a human can dance to it. So like, the machine's not gonna dance, so the machine can't have any opinion on that. The computer could have an opinion on energy. And the computer can definitely have an opinion on loudness. So there's a spectrum from, you know, loudness as purely analytical, and then energy is a little like loudness, with a little more subjectivity. And then danceability and valence are purely subjective."

Glen confirmed that this process didn't know anything about lyrics, which makes valence a particularly difficult variable to build. It could, in theory, pick up on aspects of vocal performance, but it doesn't know anything about languages or words or the meanings of songs. Take an upbeat, happy-sounding Elliot Smith song with devastating lyrics: the machine might register it as happy, where human listeners understand that it's sad. Add to that that two humans might even disagree on the song's valence.

"Plus, we have the confounding factor of like, a song that seems happy today could be sad tomorrow because the singer was killed in a plane crash. The song didn't change, but our world changed and our reactions changed."

Macdonald suggests combining energy and valence to create quadrant, which usually works "fairly well". High energy and high valence could be generally happy, cheerful, upbeat. Low energy, low valence could be sad or downbeat. High energy, low valence is sort of angry. Low energy, high valence is serene or calming.

```{r}
#| label: fig-quadvalence
#| fig-cap: Valence and Energy Quadrant
#| echo: false
#| warning: false

df <- read_csv(here::here("inputs/data/df.csv"))

ggplot(df, aes(x = energy, y = valence)) +
  geom_point(aes(color = as.factor(jack)), alpha = 0.3) +
  geom_vline(xintercept = mean(df$energy), color = "gray") +
  geom_hline(yintercept = mean(df$valence), color = "gray") +
  theme_minimal()
```

In @fig-quadvalence, it looks like songs produced by Jack Antonoff are a little more clustered towards the lower end of valence, and across the spectrum of energy, although the variance is large.

```{r}
#| label: fig-quaddanceability2
#| fig-cap: Danceability and Energy Quadrant
#| echo: false
#| warning: false

ggplot(df, aes(x = energy, y = danceability)) +
  geom_point(aes(color = as.factor(jack)), alpha = 0.3) +
  geom_vline(xintercept = mean(df$energy), color = "gray") +
  geom_hline(yintercept = mean(df$danceability),
             color = "gray") +
  theme_minimal()
```

In @fig-quaddanceability2, it looks like Jack-produced songs err towards higher danceability, across the spectrum of energy.

These quadrants are a start, but they don't suggest any defining Jack-iness characteristic underlying each of those blue points, which are scattered pretty broadly.

By comparing cumulative distribution functions of each variable based on whether a song was produced by Antonoff, we can see that some of these variables do capture some sort of underlying Jack-iness (@fig-cdf). For instance, while liveness, instrumentalness, and speechiness are relatively similar, we can see clear differences in danceability, energy, loudness, acousticness, and valence.

```{r}
#| label: fig-cdf
#| fig-cap: Cumulative Distribution Function of Each Variable, 
#| echo: false
#| warning: false

data_reduced <- read_csv(here::here("inputs/data/data_reduced.csv"))

data_reduced |>
  pivot_longer(
    cols = c(
      danceability,
      energy,
      loudness,
      mode,
      speechiness,
      acousticness,
      instrumentalness,
      liveness,
      valence,
      tempo
    ),
    names_to = "name",
    values_to = "value"
  ) |>
  ggplot(aes(x = value, color = jack)) +
  stat_ecdf() +
  facet_wrap(vars(name),
             nrow = 3,
             ncol = 4,
             scales = "free") +
  theme_minimal() +
  labs(x = "Value",
       y = "Proportion")

#even tho there's similrities, there's differences
```

Another way to gauge the Antonoff-ification of pop music could be to look at how artists' sounds have changed over time, and whether there is any correlation with Antonoff. Here is where the necessary "correlation is not causation" caveat comes in: suggesting that Antonoff is the *cause* for any change in artists' sounds would be an overstep. These plots are here to get a glimpse at pop's evolution over time, and where Antonoff fits in that picture. Each point represents a song.

```{r}
#| label: fig-energy
#| fig-cap: Energy Variable
#| echo: false
#| warning: false

df <- read_csv(here::here("inputs/data/df.csv"))

df <- df |> 
  mutate(jack = factor(jack))

df |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = energy, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Energy") +
  facet_wrap(~ artist_name, ncol = 3)
```

In @fig-energy, it looks like there's a downward trend in energy scores in artists who work with Antonoff, but that these downward trends are also somewhat in line with the artists' energy trajectories anyway. Specifically, Lana Del Rey and Taylor Swift. They have enough albums recorded prior to collaborations with Antonoff to be able to observe this. Sharon Von Etten, Maggie Rogers, and Mitski, who have never worked with Antonoff, all have upward trends.

```{r}
#| label: fig-danceability
#| fig-cap: Danceability Variable
#| echo: false
#| warning: false

df |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = danceability, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Danceability") +
  facet_wrap( ~ artist_name, ncol = 3)
```

There aren't any significant correlations in danceability (@fig-danceability) .

```{r}
#| label: fig-valence
#| fig-cap: Valence Variable
#| echo: false
#| warning: false

df |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = valence, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Valence") +
  facet_wrap( ~ artist_name, ncol = 3)
```

In @fig-valence, it looks like Lorde has the only significant change in valence scores, which tracks with my understanding her albums. Melodrama had Green Lights, and Solar Power had a music video of Lorde frolicking on a beach in a yellow bikini with Phoebe Bridgers and Clairo.

```{r}
#| label: fig-acousticness
#| fig-cap: Acousticness Variable
#| echo: false
#| warning: false

df |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = acousticness, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Acousticness") +
  facet_wrap( ~ artist_name, ncol = 3)
```

Again in @fig-acousticness, Lorde's acoustic Solar Power album jumps out at us. Taylor Swift, Lorde and Haim also have significant-seeming upward trends in acousticness, while Mitski, Sharon Von Etten and Maggie Rogers see decreasing acousticness.

```{r}
#| label: fig-instrumentalness
#| fig-cap: Instrumentalness Variable
#| echo: false
#| warning: false

df |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = instrumentalness, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Instrumentalness") +
  facet_wrap( ~ artist_name, ncol = 3)
```

In @fig-instrumentalness, St. Vincent is the only plot with any significant trend in instrumentalness.

```{r}
#| label: fig-tempo
#| fig-cap: Tempo Variable
#| echo: false
#| warning: false

df |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = tempo, x = album_release_date, color = jack)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Tempo") +
  facet_wrap( ~ artist_name, ncol = 3)
```

Finally, in @fig-tempo, there aren't many significant trends in tempo, except for Antonoff's collaborations with Lana Del Rey and Taylor Swift.

Now that we understand that there do seem to be some subtle differences in how variables that are and aren't produced by Jack behave, as seen in our cumulative distribution functions, our quadrants, and some minor trends in artists' sounds over time (specifically, `energy`), we can use a logistic regression to see if a model can a) predict whether a song was produced by Antonoff based on these variables, and b) tell us which variables were more significant in determining the output.

# Logistic Regression

## Data Preparation

I made a binary response variable, called `jack`, that has a value of 1 if the song was written by Jack, and a 0 otherwise. There are 10 predictor variables, all of which are continuous: **`danceability`**, **`energy`**, **`loudness`**, **`mode`**, **`speechiness`**, **`acousticness`**, **`instrumentalness`**, **`liveness`**, **`valence`**, and **`tempo`**.

The formula for this logistic regression can be written as:

$$
\mbox{Pr}(y_i=1) = \mbox{logit}^{-1}\left(\beta_0+\beta_1 x_i\right)
$$

where $\text{Pr}(y_i=1)$ is the probability of a song being written by Jack, $\beta_0$ is the intercept, and $\beta_1, \ldots, \beta_{10}$ are the coefficients corresponding to the predictor variables.

@fig-means gives us an idea of the mean value of each of these features for Jack and non-Jack song. A logistic regression will give us a better idea of how to weigh each of these features in order to predict the outcome, which gives us information about the "research" "question": are these characteristics related to Jack's music? (And then: Which ones? And then, the most elusive question: How?)

```{r}
#| label: fig-means
#| fig-cap: Comparing the Mean Values of Each Variable
#| echo: false
#| warning: false

data_reduced <- read_csv(here::here("inputs/data/data_reduced.csv"))

data_reduced |>
  group_by(jack) |>
  summarise(across(-c(tempo, loudness), mean)) |>
  pivot_longer(cols = -jack) |>
  ggplot(aes(value, name, fill = factor(jack))) +
  geom_col(alpha = 0.8, position = "dodge") +
  theme_minimal() +
  labs(fill = "Jack", x = "Mean Value", y = "Feature")
```

This model assumes that the relationship between the independent variables and the dependent variable is linear. There could be non-linear relationships between the variables that this model isn't capturing. Second, it assumes that the relationship between the independent variables and the dependent variable is the same for all observations, which might not be true in practice -- there could be interactions between the variables, or in certain subgroups of data where the relationship is different. Albums where Jack may have had more of a back seat producing role are one example. (The fact that a song's development, from inception to recording, is a whole creative process, each having its own origin story with varying degrees of Jack-ness, is another.)

One issue is that in our dataset, there are a lot more songs that aren't produced by Jack than songs that are. The classes have to be balanced.

```{r}
#| echo: false
#| warning: false

data_reduced |>
  count(jack) |>
  knitr::kable(col.names = c("Is it Jack?", "Number of Songs"))
```

After some minor pre-processing using the `tidyverse` R package [@citetidyverse]. I followed @citeSilge to build a model that deals with the imbalanced classes.

I used the `tidymodels` to fit the model [@citetidymodels]. To deal with the imbalanced classes, I downsampled the majority class using the **`step_downsample()`** function from the **`themis`** package. Then I used 5-fold cross-validation with stratification by the **`jack`** variable to estimate the performance of the model.

The `workflow_set()` function lets us hold different sets of feature engineering together and use tuning to evaluate them all at once. I need to evaluate two elements: a basic model, and a model that was downsampled using the `themis` package.

I used **`rank_results()`** from the **`yardstick`** package to rank the models based on accuracy, log loss, sensitivity, and specificity, and went with the downsampled model.

## Results

The model's accuracy score is 0.743, and the ROC AUC score is 0.841. The accuracy score could be misleading, since the model had imbalanced classes, and downsampling may have affected the model's ability to correctly identify the minority class. The ROC AUC score is more reliable, since it's less sensitive to class imbalance.

|                        | Actually Positive (1) | Actually Negative (0) |
|------------------------|-----------------------|-----------------------|
| Predicted Positive (1) | 581                   | 25                    |
| Predicted Negative (0) | 194                   | 51                    |

: Confusion Matrix for Logistic Regression Results

@tbl-importance shows the different variables' importance on determining whether a song was had any Antonoff involvement.

```{r}
#| label: tbl-importance
#| tbl-cap: Importance Scores of Each Variable in Determining Jack-ness
#| echo: false
#| warning: false

jack_vip <- read_csv(here::here("inputs/data/jack_vip.csv"))

jack_vip |>
  knitr::kable(col.names = c("Variable",
                             "Importance",
                             "Sign"))
```

```{r}
#| label: fig-importance
#| fig-cap: Importance Scores of Each Variable in Determining Jack-ness
#| echo: false
#| warning: false

jack_vip |>
  group_by(Sign) |>
  slice_max(Importance, n = 15) |>
  ungroup() |>
  ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) +
  geom_col() +
  facet_wrap(vars(Sign), scales = "free_y") +
  labs(y = NULL) +
  theme(legend.position = "none")
```

Danceability, with a positive importance score of 4.32, had the highest level of importance in determining whether a song was produced or co-produced by Jack Antonoff. Instrumentalness comes in second at 2.59, with a negative score (meaning that negative instrumentalness is correlated), and valence comes in third with a negative score at 2.02.

```{r}
#| label: tbl-mostdanceable
#| tbl-cap: Top 20 Most Danceable Antonoff-Produced Songs
#| echo: false
#| warning: false

df |>
  filter(jack == 1) |>
  arrange(desc(danceability)) |>
  distinct(track_name, .keep_all = TRUE) |>
  select(artist_name, danceability, track_name) |>
  head(20) |>
  knitr::kable(col.names = c("Artist Name",
                             "Danceability",
                             "Track"))
```

@tbl-mostdanceable shows us the 20 songs in Jack Antonoff's discography with the highest danceability scores. Bleachers takes up 35% of this list, and Taylor Swift takes up 40% of this list. St Vincent, The Chicks, Lorde, and Florence and the Machine also haves songs up there.

# Random Forest

Random forests (RF) build an ensemble of decision trees during the training phase, and then use majority voting for classification tasks. Each tree in the ensembled is trained on a random sample of data taken with replacement, and only a random subset of features are considered when splitting at each node. This method ensures that each tree in the ensemble is unique, avoiding overfitting and capturing non-linear relationships and more complex patterns in the data. The formula behind the RF is:

## Data Preparation

The categorical feature 'producer', which contains the names of the producer for each track, was one-hot encoded to create binary columns for each producer ('is_antonoff', 'is_elworth', etc.). The producer feature was then converted into a factor variable, and the dataset was split into training and tests sets.

## Model Training

The RF was trainied using the @ranger package. The 'importance' parameter was set to 'impurity', which sets the model to interpret variable importance using Gini impurity.

Gini impurity is a measure of misclassification. At every split in a decision tree, the algorithm tests how well each feature splits the data. The features that split the data the most accurately, and then with the least impurity, would have the lowest impurity score. In Random Forests, a variable's importance can be calculated by looking at how much the tree nodes that use that variable reduce impurity on average.

Gini impurity was chose on the grounds that this study is interested in not only classifying producers, but, more importantly, in understanding how the underlying data informs these decisions. By calculating the Gini impurity not only for the entire model, but specifically for each producer, we can better understand how the variables in our dataset describe the differences between producers. Without song structure, instruments, sound, or other more ephemeral components of style, these differences can be taken as a proxy for style.

## Results

### Model Performance

| Metric                  | Value           |
|-------------------------|-----------------|
| Accuracy                | 0.6111          |
| 95% Confidence Interval | (0.579, 0.6425) |
| No Information Rate     | 0.4274          |
| P-Value (Acc \> NIR)    | \<2.2e-16       |
| Kappa                   | 0.4223          |

: Overall Performance Statistics

### Stylistic Difference

Correlation heatmaps can help us better understand the differences between each producer's "style". The variables with the most variance were danceability, loudness, and energy.

```{r}
#| echo: false
#| warning: false
## Range of correlation scores

library(dplyr)

range_data <- table_data %>%
  gather(variable, value, -producer, -term) %>%
  group_by(variable) %>%
  summarize(
    min_value = min(value, na.rm = TRUE),
    max_value = max(value, na.rm = TRUE),
    range = max_value - min_value
  ) %>%
  arrange(-range)


##Variables with greatest variance##
top_vars <- range_data %>% 
  top_n(3, range) %>%
  pull(variable)

print(top_vars)



selected_data <- table_data %>%
  select(producer, term, all_of(top_vars)) %>%
  arrange(producer, term)

print(selected_data)

library(ggplot2)
```

Similar to the logistic regression, danceability was also, across all producers, the most important predictor in the classification process.

```{r}
#| echo: false
#| warning: false
list_importance <- list()
unique_producers <- unique(data_reduced$producer)

for (producer in unique_producers) {
  data_temp <- data_reduced
  data_temp$producer_binary <- as.factor(ifelse(data_temp$producer == producer, 1, 0))
  
  rf_temp <- randomForest(producer_binary ~ . - producer, data = data_temp, ntree = 100)
  
  importance_temp <- rf_temp$importance
  print(colnames(importance_temp))  # To see available columns
  
  list_importance[[producer]] <- importance_temp[,'MeanDecreaseGini']
}

df_importance <- bind_rows(list_importance, .id = "producer")

head(df_importance)

df_importance_long <- df_importance %>%
  gather(variable, importance, -producer)

ggplot(df_importance_long, aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity") +
  facet_wrap(~producer, scales = "fixed") + 
  coord_flip() +
  labs(title = "Variable Importance by Producer", 
       x = "Variable", 
       y = "Mean Decrease Gini") +
  theme_minimal()


```

In the visualization of danceability correlations across producers, Antonoff's danceability has the least positive correlation with valence (SCORE), the most negative correlation with acousticness (SCORE), and significant positive correlations with speechiness (SCORE), loudness (SCORE), and energy (SCORE).

```{r}
#| echo: false
#| warning: false
# Danceability Plot
ggplot(selected_data, aes(x=term, y=danceability, fill=producer)) +
  geom_bar(stat="identity", position="dodge") +
  labs(title="Danceability across Producers", y="Danceability Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Discussion

Antonoff's danceability score being, compared to other producers, significantly positively correlated with speechiness, loudness and energy, negatively correlated with acousticness, and less than average with valence tracks with how critics tend to describe his music.

His tracks have been described as anthemic (cite), with a distinct vocal treatment that makes it sound, as Gamman put it, "the way it would sound to the person who's singing them".

> In the treble mix of the song, like in the very upper end, sort of above where the vocals are -- he often really crushes that down. It's lower in volume than anything else, which is sort of a weird effect. When you hear someone speaking, you hear a lot of that, like, noise in their voice. When you yourself are speaking, you hear less of it, right? You hear more of your own voice bouncing around in your head. And so it sort of creates this effect of -- **like he mixes his vocals the way it would sound to the person who's singing , them. Which is sort of strange.** He often has no vocals on that upper end, which is very unusual, and then often there are random little bits of noise happening up there which is the sort of thing her likes to do.

Clearing noise and isolating the middle range of the vocals may make for a stronger "speechiness" feature.

Valence, like danceability, was also a subjective variable built using labeling. Returning to the music itself, Antonoff-produced songs with the highest danceability scores aren't exactly dance hits.

-   I Think He Knows by Taylor Swift:

-   Vigilante Shit by Taylor Swift:

    -   Not danceable.

-   Heaven is Here by Florence and the Machine

    -   Not danceable, but it does seem like it's trying to get your body to do something.

-   I Wanna Get Better by Bleachers

    -   More anthemic than danceable. It also seems like it's trying to coercing your attention, if not your body, and the speechiness, loudness, and energy are all apparent. The valence is apparent in the lyrics, but it doesn't have the kind of rhythm for dancing.

-   Cornelia Street by Taylor Swift

    -   

## Danceability Predicts Jack-iness {#sec-first-point}

Using the Spotify Audio Features, you can't really gauge what Jack Antonoff's music *sounds* like. You get a rough idea, but you don't get the same granularity that you would by simply DM'ing Caleb Gamman on Twitter and asking him to describe it you. Or by watching the video where he already did that.

> I think it's mostly in the vocal treatment, that's the thing that is the most off-putting to me. But just every decision feels a little bit wrong to me. If I wanted to be cringe I could point out the Juno 6 Bass, the perfectly stacked vocals, the Juno 6 pad, the wide and tight plate reverb, the Juno 6 sword, the goo goo gaga, the man hates treble unless it's just random crap swishing around, everything gets low passed with thin 707 drum samples, thin 808 drum samples, then 909 drum samples, thin Lin drum samples, thin thin drum samples, all the resonances, the harmonics get removed, everything is sort of squished down to the fundamental reduction in timbre, everything exists in a small frequency range, everything becomes a tone, it's an extremely consonant sound with no interplay between the different elements. Now that is a skill to do, and it produces a cleaner sound, and the only problem is I viscerally hate that sound.

According to this model, though, danceability is the most important feature in predicting whether a song was produced by Jack Antonoff. And the accuracy metrics aren't awful -- the accuracy score is 0.743, and the roc_auc (receiver operating characteristic area under the curve) (more fun to say out loud as an abbreviation. Rock Auck), which is a metric better suited for imbalanced datasets, such as our own, is 0.810. This means that although danceability can't show us the inside of a song, something about this score is telling us something about Jack's music.

### In which I do a close listen of our model's top 5 most danceable songs[^3]:

[^3]: I encourage you to follow along and drop me an email with your own assessment at michaela.drouillard\@mail.utoronto.ca.

-   I Think He Knows by Taylor Swift:

    -   Pretty danceable tbh.

-   Vigilante Shit by Taylor Swift:

    -   Not danceable. But it is trying to coerce you into some sort of body feeling.

-   Heaven is Here by Florence and the Machine

    -   DEFINITELY not danceable. But it is trying to get your body to do something. Kind of sounds like it's suggesting that you join a Macedonian phalanx or something. But no. Not danceable.

-   I Wanna Get Better by Bleachers

    -   I don't know if "danceability" is the word, but I'll give it "anthem"-y. Like, I can't picture a group of friends dancing to this in the kitchen, but I can picture someone driving, on their way to give a presentation or negotiate a promotion or something, and this song comes on the radio and they leave it on. They kind of lean into it.

-   Cornelia Street by Taylor Swift

    -   Yeah I'm dancing. Or at least bobbing my head. I only became aware of that fact when I made eye contact with the guy next to me at this cafe.

I am thinking about what Gamman said in his video.

> I did use the phrase "visceral hatred". But i did so only to describe my visceral hatred. I can't stand the sound of it. I hate it. And the way in which I hate it is viscerally. But I didn't go beyond that because it doesn't go beyond that. Nothing against the guy, I like Taylor swift's stuff. It's just subjective taste.

It's just his subjective taste, but in one way Gamman was actually totally on the money about this. The danceability variable, which is the strongest predictor of Jack-ness, seems to actually be more about "visceral"-ness when you do a close-listen of the songs with the highest danceability scores. Not all of them are danceable, but the one's that aren't still feel like they're trying to manipulate the way your body is holding itself in some way. The reason Jack is getting under people's skin could just be because his music... gets underneath your skin. Right to the viscera.

## Weaknesses and next steps

One fundamental weakness is cooked into the dataset. Since we're not in the studio, we can't know the degree of Jack-iness in each individual song. Especially when he takes on the role of a more backseat co-producer. We can't actually know what his involvement in each song was. There's also the issue that a study like this implies that Jack Antonoff is the reason for the way these artists' sounds develop, which takes that agency away from the artists. This is a critique you see on the internet, and also a critique that some people on Twitter leveraged against Gamman after his Midnights video went viral. I mentioned it to a friend when she asked me how my final paper was going, and she said something like: "Yeah but like, it's *Taylor Swift*. As if anyone actually thinks that Jack Antonoff is the reason for her success." I agree.

#### Could our model just be predicting Jack via Taylor Swift?

So another issue is that there is a lot of Taylor Swift in our training data. As mentioned earlier, her songs take up 40% of @tbl-mostdanceable, which is 5% more than Antonoff's own band, Bleachers. Taylor Swift has the second highest average danceability score across all songs, and the most danceable songs are hers. She also happens to have a lot of collaborations with Antonoff under her belt -- more than any other artist. I suspect that perhaps our model was predicting Jack via Taylor, which could be a beautiful story about collaboration and aesthetic affinity. Or it could suggest that this model has a stink to it. An avenue for future study.

```{r}
#| echo: false
#| warning: false

df |>
  filter(jack == 1) |>
  ggplot(aes(x = artist_name, y = danceability)) +
  geom_boxplot() +
  ggtitle("Boxplot of Danceability by Artist") +
  coord_flip() +
  theme_minimal()
```

```{r}
#| echo: false
#| warning: false

df |>
  filter(jack == 1) |>
  count(artist_name) |>
  arrange(desc(n)) |>
  head(10) |>
  ggplot(aes(x = reorder(artist_name, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Artists with Most Jack = 1 Values", x = "Artist", y = "Count") +
  theme_minimal()

```

#### Next Steps

It only makes sense, of course, to hire a bunch of undergrads and get them to tell us whether a few thousand songs are visceral, and then build our own visceral-ness metric, and see if we get the same results -- that danceability is just visceral-ness in a trench coat, and that visceral-ness is the characteristic that helps us best understand what makes Jack, Jack.

And if we're doing this for danceability, there might also be something worth investigating in valence, the feature that indicates a song's "positivity", which was also built via college interns' intuitions.

\newpage

# References
