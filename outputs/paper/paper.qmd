---
title: "Jack Antonoff-ification of pop music"
subtitle: "My subtitle if needed"
author: 
  - Michaela Drouillard
thanks: "Code and data are available at: LINK."
date: today
abstract: "In this essay, I will be exploring Jack Antonoff's production style using data from the Spotify API. Drawing from an interview with Caleb Gamman, a guy who went viral for guessing which song on Taylor Swift's Midnights album was produced by Antonoff within seconds, I will delve into the specific characteristics that define an Antonoff-y song, and assess whether or not the audio features defined by Spotify can capture this. I plan to lasso Glen McDonald,the Spotify engineer behind these features, into explaining them to me. There will be a Bayesian inference model. It will be a logistic regression. We do not know whether or not it will be good. Finally, I will armchair a little bit about the cultural significance of Antonoff's music and the reasons behind his ubiquity in the music industry."
format: pdf
number-sections: true
bibliography: references.bib
---

# Introduction

Sometime in early October of 2022 there was a chemical shift in my brain and I couldn't stop listening to Taylor Swift. I couldn't listen to anything other than Fearless or Red when deadlifting, walking to a friend's house, folding laundry, etc. I listened to so much Taylor in that month alone that she ended up as my #4 artist on Spotify Wrapped, which stops collecting its data for the year on October 31st. It didn't even capture what happened to me after Midnights dropped in November.

I was standing outside of the Garrison eating a slice of pizza with some friends, and one of them asked what music we had been listening to lately.

ME: "Midnights!"

ZACH: "Is it good?"

\[I glitched.\]

ME: "No!"

\["*No*?"\]

A STRANGER WALKING BY WHO HEARD EVERYTHING: "EXACTLY!!"

Midnight was produced by Swift and her longtime collaborator Jack Antonoff, who is the subject of this paper, and, I suspect, the reason behind both my blacking out, and the stranger's "Exactly".

Antonoff is prolific, to the extent that it is starting to bother people. He has produced and co-produced music for Lorde, Taylor Swift, Florence and the Machine, The Chicks, Clairo, The 1975, Grimes, Zayn, Pink, Lana Del Rey, Olivia Rodrigo, the Minions: The Rise of Gru Soundtrack, and more. All of this and his own bands too (Bleachers, and fun.). When I was getting brunch with a friend one Sunday and a Swift song came on, and she said "Jack Antonoff's gonna kill us all", we both laughed. What my friend was referring to, of course, is the fact that Jack Antonoff and his sound seems to be everywhere these days. As Andrew Marantz writes in the New Yorker: "When his band releases an album, the world responds politely. When he produces one by Lorde or Lana Del Rey or Taylor Swift, the world wobbles on its axis".

What I am trying to get to here is that I don't actually really mind Jack Antonoff's music, I kind of like it, but I still say his voice with the same tone that everyone else does. There's just something about him... this sense that he's everywhere... and that he's just such a nice guy... Why do music people seem to be a little annoyed with him? What is it about Antonoff music that is so Antonoff-y? Is the Jack Antonoff-fication of pop something we should be worried about?

To find an answer to this question, I turned to statistics, people who know more than me about describing music, and the overlord of engineered virality itself -- the Spotify API. There will be a Bayesian inference model. It will be a logistic regression. We do not know whether or not it will be good.

# An Interview with Caleb Gamman

Around the time that Midnights came out, I came across a video on Twitter of a guy, Caleb Gamman, guessing which songs on Midnights were produced by Antonoff within seconds. The tweet reads: "im able to instantly detect if jack antonoff worked on a song due to a visceral hatred of his production style". He nails it. Only one song on the album wasn't produced by Antonoff, and he guesses it.

I reached out to Gamman to ask him to explain Antonoff to me, only to find out, on the morning of the interview, that Gamman had released a 29 minute video essay on his Youtube Channel covering:

(you can skim the quote chunks)

1.  what makes an Antonoff song so Antonoff-y

    > I'm literally not an expert and of course I don't actually claim to always be able to 100 identify a Jack Antonoff song it's not like I've been practicing the way that I can tell if he's produced something is basically if it sounds to me like he's produced it I know that's annoying I will clarify first I think it's mostly in the vocal treatment and that's the thing that is the most off-putting to me but just every decision uh feels a little bit wrong to me I don't know of course if I wanted to be cringe in CinemaSins I could point out the Juno 6 Bass the perfectly stacked vocals the Juno 6 pad the wide and tight plate Reverb the Juno 6 sword the goo goo gaga the man hates treble unless it's just random crap swishing around everything gets low passed with thin 707 drum samples thin 808 drum samples then 909 drum samples thin Lin drum samples thin thin drum samples all the resonances the harmonics get removed everything is sort of squished down to the fundamental reduction in Timbre everything exists in a small frequency range everything becomes a tone it's an extremely consonant sound with no interplay between the different elements now that is a skill to do and it produces a cleaner sound and the only problem is I viscerally hate that sound

2.  his relationship to music growing up (some gems in here)

    > hing I got into drumming at the age oflike six so maybe as a drummer the drumsamples are of a fence I wasanachronistically raised on cassettes soyou know maybe I like the sound ofaudio and maybe that explains I tend tolike dissonance and in harmonic soundinterplay of elements Distortion noisethe Clashing of elements I imagine I'velistened to more SoundCloud in my lifethan radio so uh maybe I only want tohear amateur internet loser I'veliked over 10 000 songs on my Spotifyaccount uh and I can identify lot ofthem off the bat but I'm not that greatat identifying music generally so it'sjust uh compartmentalized mediaconsumption habit I first did musicproduction in primary school I mademashups and dubstep in it as a lateteenager I extensively used Juno 6emulators to make embarrassing synthwaveadmittedly that's probably not thesubconscious Association that he's goingfor lyrics aren't of much interest to meit's just a sound in the mix and somaybe I don't like the focus on vocalI've previously described it as a fightor flight sort of thing so maybe itliterally is tapping into some memory oftrauma I have an unusually small earcanal so maybe I'm just hearing the highend more acutely I was hired as ateenager to play drums on uh a lot ofJack Antonoff songs for a universityGlee Club performer so maybe I don'tlike it because of the embarrassment ofhaving ever been associated with a Gleecloth I liked the album 1989 but to makethe song out of the woods palette ofwool to me I had to mash it up with M83Midnight City and that got a dmcatakedown so maybe it's a personalVendetta and of course I have known anddo know people who like this music somaybe I secretly hate my friends andfamily but I don't know if any of thattells us anything uh I have a taste inmusic w

3.  An elegant and simple defense of taste

    > "i did use the phrase visceral hatred. but i did so only to describe my visceral hatred. i cant stand the sound of it. i hate it. and the way in which i hate it is viscerally. but i didnt go beyond that because it doesnt go beyond that. nothing against the guy, i like taylor swift's stuff. it's just subjective taste...

4.  His tongue in cheek response to some of Twitter's negative response to his video

    > .... \[it's just subjective taste\]. there's just nothing to engage with, so ive had to fabricate a reason for controversy. and what ive come up with is this. \*looks at phone\* the real reason iive been saying for years that i have a visceral reaction to jack antonoffs production style, the REAL reason is this \-- cause during the rollout cycle of the album midnights, i wanted to ascribe taylor swifts authorship to a man and diminish women's voices. so actually, i'm a sexist. so now thats out there, publishhed as facts, in the news. with all the other facts..."

5.  Basically a refutation to the entire premise of my interview

    > "what makes you so special and unique that you can tell the production of a song?" "nothing"

6.  Which was refuted even harder when I found a screenshot he tweeted of his tweet drafts in June 2021, one of them reading: "people often say to me:"caleb, what's wrong with jack antonoff?". don't ask questions"

![](images/Screen%20Shot%202023-04-03%20at%209.31.05%20PM.png)

It is a perfect video essay. The only thing left that I could ask was how he felt about that video 6 months after the fact, and what he thought of me asking him about it.

\[\[He told me that he didn't actually refute the premise of my interview, and ended up saying a lot of interesting things, which i will transcribe and somehow make use of in this paper later\]\]

# Numbers

TLDR of the Gamman interview is: Taste is subjective. People who can play instruments tend to be better at identifying and describing sounds. Cultural criticism is a hall of smoke and mirrors. Antonoff is everywhere.

Can we translate any of this into something observable? Can we quantify the Jack Antonoff-ification of pop music?

# Data {#sec-data}

I pulled the data using the Spotify API, which I accessed using the `spotifyr` package in R. I used the `get_artist_audio_features` and `get_track_audio_analysis functions` function to acquire data on Lorde, Taylor Swift, St. Vincent, Lana Del Rey, The Chicks, Florence and the Machine, and Bleachers discographies.

### Artist Selection

I chose these artists because they've produced multiple albums, and at least one album with Antonoff, as opposed to just individual songs. I chose the Chicks because I figured that collaborating with Antonoff after an extended gap in a longer spanning career than the other artists might offer some interesting results. The data points at beginning of their career reflect music that existed in a different time from the other artists in this dataset.

### Data Cleaning

I joined these data sets using the `rbind` function in base R, and created an additional variable called "jack", which contains a 1 if Jack Antonoff produced or co-produced the song, and a 0 if he had nothing to do with it. I got the information on which songs Antonoff did or did not produce from the "Jack Antonoff Production Discography" Wikipedia page.

### Spotify's API

So what do any one these features mean?

The Spotify API returns JSON metadata directly from the Spotify Data Catalogue.

#### Audio Features Dataset

The `get_artist_audio_features` provides a general profile of each song, according to variables developped by spotify, which makes for simple comparisons across artists.

However, it's hard to gauge exactly what variables like "danceability" or "valence" mean, even using the documentation. For instance, "valence" is described as "A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)". "Danceability" is described as "how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable". Neither of these definitions fully cover what metrics and thresholds go into something like "danceability". However, whatever these features capture is easier to compare across artists in visualizations than more granular details about bars, beats, duration, and tatums[^1].

[^1]: \-- a word I just learned, which refers to "the lowest regular pulse train that a listener intuitively infers from the timing of perceived musical events". The full documentation for each variable included in this study is available in the Appendix

This is pretty vague. Especially valence -- is the positivity related to lyrics, or something else? I compared the valence scores of classical music tracks to see if this score is based on lyrics, or the audio itself. Jupiter's valence score is 326.78% higher than Mars. This not only indicates the valence is based on audio features other than lyrical content, but that it is in fact capturing a difference between the two songs' profiles.

```{r}

#| echo: false
#| warning: false
library(dplyr)

sub_planets<- read.csv(here::here("inputs/data/subset_planets.csv"))
sub_planets |>
  select(artist_name, valence, danceability, track_name) |>
  head() |>
  knitr::kable(
    col.names = c("Artist Name", 
                  "Valence", 
                  "Danceability",
                  "Track Name")
 )


```

I looked at the valence measures on Flight of the Bumblebee, a song that's much faster and "upbeat" than Mars and Jupiter. What I found complicated my understanding of valence -- different performances of the same song had different valence scores. Some were arias, played by different instruments. These tend to have higher valence scores. The lowest recorded valence is 0.2410, and the highest is .9230. What gives?

```{r}
sub_bees<- read.csv(here::here("inputs/data/subset_bumblebee.csv"))
sub_bees |>
   select(artist_name, valence, danceability, album_release_year, track_name) |>
   head(17) |>
   knitr::kable(
     col.names = c("Artist Name", 
                   "Valence",
                   "Danceability",
                   "Album Release Year",
                   "Track Name")
  )



```

The final audio features in this study data set contains the variables: "artist_name", "track_name", "energy", "danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "jack".

```{r}
#| echo: false
#| warning: false

#library(kableExtra)
#df |>
 # knitr::kable()

```

#### Audio Analysis Dataset

The `get_track_audio_analysis` offers a more granular look at each track. The variables included are arrays of objects, which are the kinds of things that are a bit harder to wrap your mind around if you were to visualize each one across each artist. They include: track, bars, beats, sections, segments, tatums, and meta, which includes information on which platforms and Analyzer versions were used to analyze the track, along with metadata on the status and duration of the analysis.

This data will be used when building a logistic regression model to determine the character and predictability of Jack Antonoff's music.

# Model

$$
Pr(\theta | y) = \frac{Pr(y | \theta) Pr(\theta)}{Pr(y)}
$$ {#eq-bayes}

My model is driving me bananas.

# "EDA"

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
```

```{r}
#| label: fig-energy
#| fig-cap: Energy Variable
#| echo: false
# 
# library(lubridate)
# df <- read.csv(here::here("inputs/data/df.csv"))
# df %>%
#   filter(album_release_date_precision == "day") %>%
#   mutate(album_release_date = ymd(album_release_date)) %>%
#   ggplot(aes(y = energy, x = album_release_date, color = jack)) +
#   geom_point() +
#   theme_minimal() +
#   scale_color_manual(values = c("#FFB6C1", "#808080")) +
#   labs(color = "Has 'y' in jack?") +
#   facet_wrap(~ artist_name, ncol = 3)



```

```{r}
#| label: fig-danceability
#| fig-cap: Danceability Variable
#| echo: false


# df %>%
#   filter(album_release_date_precision == "day") %>%
#   mutate(album_release_date = ymd(album_release_date)) %>%
#   ggplot(aes(y = danceability, x = album_release_date, color = jack)) +
#   geom_point() +
#   theme_minimal() +
#   scale_color_manual(values = c("#FFB6C1", "#808080")) +
#   labs(color = "Has 'y' in jack?") +
#   facet_wrap(~ artist_name, ncol = 3) 




```

```{r}
#| label: fig-valence
#| fig-cap: Valence Variable
#| echo: false


# df %>%
#   filter(album_release_date_precision == "day") %>%
#   mutate(album_release_date = ymd(album_release_date)) %>%
#   ggplot(aes(y = valence, x = album_release_date, color = jack)) +
#   geom_point() +
#   theme_minimal() +
#   scale_color_manual(values = c("#FFB6C1", "#808080")) +
#   labs(color = "Has 'y' in jack?") +
#   facet_wrap(~ artist_name, ncol = 3) 





```

```{r}
#| label: fig-acousticness
#| fig-cap: Acousticness Variable
#| echo: false


# df %>%
#   filter(album_release_date_precision == "day") %>%
#   mutate(album_release_date = ymd(album_release_date)) %>%
#   ggplot(aes(y = acousticness, x = album_release_date, color = jack)) +
#   geom_point() +
#   theme_minimal() +
#   scale_color_manual(values = c("#FFB6C1", "#808080")) +
#   labs(color = "Has 'y' in jack?") +
#   facet_wrap(~ artist_name, ncol = 3) 





```

```{r}
#| label: fig-instrumentalness
#| fig-cap: Instrumentalness Variable
#| echo: false

# 
# df %>%
#   filter(album_release_date_precision == "day") %>%
#   mutate(album_release_date = ymd(album_release_date)) %>%
#   ggplot(aes(y = instrumentalness, x = album_release_date, color = jack)) +
#   geom_point() +
#   theme_minimal() +
#   scale_color_manual(values = c("#FFB6C1", "#808080")) +
#   labs(color = "Has 'y' in jack?") +
#   facet_wrap(~ artist_name, ncol = 3) 





```

```{r}
#| label: fig-tempo
#| fig-cap: Tempo Variable
#| echo: false

# 
# df %>%
#   filter(album_release_date_precision == "day") %>%
#   mutate(album_release_date = ymd(album_release_date)) %>%
#   ggplot(aes(y = tempo, x = album_release_date, color = jack)) +
#   geom_point() +
#   theme_minimal() +
#   scale_color_manual(values = c("#FFB6C1", "#808080")) +
#   labs(color = "Has 'y' in jack?") +
#   facet_wrap(~ artist_name, ncol = 3) 





```

# Results

# Discussion

## First discussion point {#sec-first-point}

## Second discussion point

## Third discussion point

## Weaknesses and next steps

\newpage

\appendix

# Appendix {.unnumbered}

### `get_artist_audio_features` documentation:

-   **acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.

-   **danceability**: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.

-   **energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.

-   **instrumentalness**: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.

-   **key**: The key the track is in. Integers map to pitches using standard [Pitch Class notation](https://en.wikipedia.org/wiki/Pitch_class). E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.

-   **liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.

-   **loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.

-   **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.

-   **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.

-   **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

# Additional details

\newpage

# References
