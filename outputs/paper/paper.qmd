---
title: "Detecting Stylistic Variation in Pop Production"
author: 
  - Michaela Drouillard
thanks: "Code and data are available at: https://github.com/michaeladrouillard/spotify_pop."
date: today
abstract: "We explore whether we can predict if a song was produced by Jack Antonoff using the audio features provided by Spotify's API. Our logistic regression model has an accuracy score of 76%, a precision score of 39%, and a recall of 76%. Our random forestmodel has an accuracy of 61%, and a No Information Rate of 43%. We especially found that danceability, a metric trained on human-tagged data, was a key predictor. Our results have implications for computational explorations of culture, how the combination of human subjectivity and machine listening can come to create an approximate for style."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| echo: false
#| warning: false

library(tidyverse)
library(lubridate)
library(tidyr)
library(dplyr)
library(tidymodels)
library(randomForest)
library(ranger)
library(caret)
library(themis)
library(yardstick)

```

# Introduction

We are interested in understanding how to create an approximate for production style. To do this we used Spotify API's audio features dataset to explore different audio feature metrics and identify which were the most import predictors in identifying pop producers' style. We focus on Jack Antonoff, as he is a prolific and widely used producer, who also has his own solo acts.

By way of background, Antonoff began his career as a guitarist in fun., and is the frontman in Bleachers. He has produced and co-produced music for Lorde, Taylor Swift, Florence and the Machine, The Chicks, Clairo, The 1975, Grimes, Zayn, Pink, Lana Del Rey, Olivia Rodrigo, the Minions: The Rise of Gru Soundtrack, and more. His style is considered identifiable. As Andrew Marantz writes in the *New Yorker*: "When his band releases an album, the world responds politely. When he produces one by Lorde or Lana Del Rey or Taylor Swift, the world wobbles on its axis" [@citeMarantz].

We are interested in whether Spotify audio features can be used to train a model that distinguishes whether a song was produced by Antonoff. We obtained tracks from six artists who've collaborated with Antonoff, both before their collaboration and during. We also obtained tracks from similarly prolific pop producers: Joel Little, Ariel Rechstaid, Max Martin, Greg Kurstin, Paul Epworth and Rick Nowels. Our data set contains 3738 tracks in total.

We built two models to explore the data. The first model, a logistic regression, draws on the dataset of artists who have collaborated with Antonoff, with a binary variable to tag whether or not the song was an Antonoff production. With this model, we explore which variables are stronger predictors of Antonoff's style, developping a better understanding of whether the Spotify data can trace Antonoff's influence on an artist's sound over time. We built a second model, a Random Forest (RF), to complement the insights drawn from the logistic regression. The RF trains on the full data set, including other major pop producers.

We find that `danceability` is the strongest predictor of whether or not a song has been produced by Antonoff. Furthermore, when `danceability`'s correlation scores with other features in the dataset are considered, the variance in correlation scores tends to be strongest classifier of whether a song was produced by Antonoff. Danceability in concert with other features in the dataset provides the best predictions.

Our results suggest that we can capture something about Antonoff's music with these variables, and it might, in the end, just be that subjective "ick" that some people feel, and others enjoy, and that we're all just trying to describe in using different methods and tools. The remainder of this paper is structured as follows:

# Data {#sec-data}

We collected data from the Spotify API, which we accessed using the `spotifyr` package in R [@citeSpotify]. We acquired data on Lorde, Taylor Swift, St. Vincent, Lana Del Rey, The Chicks, Florence and the Machine, and Bleachers discographies. HAIM, Marina and the Diamonds, Maggie Rogers, Sharon Von Etten, and Mitski were included to represent other contemporary pop artists with overlapping fan bases who have never collaborated with Antonoff. Of the artists who did collaborate with Antonoff, we chose artist who have produced multiple albums, and at least one album with Antonoff.

We acquired the discographies of comparable contemporary pop producers' by scraping their respective discography Wikipedia pages. The producers include Epworth, Rechstaid, Max Martin, Nowels, Joel Little, and Greg Kustin. These producers were chosen based both on their popularity, and because some have collaborated with the same artists as Antonoff.

The data was manually validated, and live performances, karaoke editions, international versions or translations, and remix albums were removed. Where deluxe albums were available, original albums were deleted to avoid duplicate rows.

We created a binary variable which contains 1 if Jack Antonoff produced or co-produced the song, and 0 if it was produced by somebody else. The information underpinning this discography was drawn from the "Jack Antonoff Production Discography" Wikipedia page [@citewiki].

In total, we collected 3738 tracks from 828 different artists. If tracks were produced by producers other than the main producers included in our study, they were marked as "other".

```{r}
#| label: tbl-producercounts
#| tbl-cap: Counts of Tracks Per Producer in Original Dataset
#| echo: false
#| warning: false
lr_data<- read_csv(here::here("inputs/data/lr_data.csv"))

library(kableExtra)

tbl <- table(lr_data$producer)

tbl |>
  kable(col.names = c("Producer", "Number of Tracks"),
  digits = 1,
  booktabs = TRUE,
  linesep = "")



```

### Audio Features Dataset

Spotify provides variables for each song, according to variables developed by Spotify, which makes for simple comparisons across artists. The final audio features in our data set are: `artist_name`, `track_name`, `energy`, `danceability`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `jack`.

From the `get_artist_audio_features` documentation (@citeSpotifyDocumentation):

> -   **acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
>
> -   **danceability**: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
>
> -   **energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
>
> -   **instrumentalness**: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
>
> -   **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.
>
> -   **key**: The key the track is in. Integers map to pitches using standard [Pitch Class notation](https://en.wikipedia.org/wiki/Pitch_class). E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.
>
> -   **liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.
>
> -   **loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.
>
> -   **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.
>
> -   **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
>
> -   **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

```{r}
#| label: fig-means
#| fig-cap: Comparing the Mean Values of Each Variable
#| echo: false
#| warning: false

lr_data <- read_csv(here::here("inputs/data/lr_data.csv"))


lr_data %>%
  select(-producer, -tempo, -loudness) %>%  
  pivot_longer(cols = -is_antonoff, names_to = "feature", values_to = "value") %>%
  ggplot(aes(x = value, y = feature, color = factor(is_antonoff))) +
  geom_point(alpha = 0.8, position = position_jitter(width = 0.2)) +
  scale_color_manual(values = c("blue", "orange"), labels = c("No", "Yes")) +
  theme_minimal() +
  labs(color = "Antonoff", x = "Value", y = "Feature")


  
```

By analyzing the cumulative distribution functions of various audio features, particularly when categorizing songs based on whether they were produced by Jack Antonoff (@fig-cdf), distinct patterns emerge. Notably, while attributes such as liveness, instrumentalness, and speechiness exhibit relative homogeneity, there are slight disparities in features like danceability, energy, loudness, acousticness, and valence.

```{r, fig.width= 8, fig.height=6}
#| label: fig-cdf
#| fig-cap: Cumulative Distribution Function of Each Variable 
#| echo: false
#| warning: false
av<- read_csv(here::here("inputs/data/antonoff_viz.csv"))

av$is_antonoff <- ifelse(av$producer == "antonoff", 1, 0)
av$is_antonoff <- as.factor(av$is_antonoff)
 av |>
   pivot_longer(
     cols = c(
       danceability,
       energy,
       loudness,
       mode,
       speechiness,
       acousticness,
       instrumentalness,
       liveness,
       valence,
       tempo
     ),
     names_to = "name",
     values_to = "value"
   ) |>
   ggplot(aes(x = value, color = is_antonoff)) +
   stat_ecdf() +
   facet_wrap(vars(name),
              nrow = 3,
              ncol = 4,
              scales = "free") +
   theme_minimal() +
   labs(x = "Value",
        y = "Proportion")


```

Examining the temporal evolution of artists' sounds, especially in correlation with their collaboration with Antonoff, provides insights into the broader trends in pop music. Each point in the following plots represent one track.

```{r, fig.width= 8, fig.height=6}
#| label: fig-energy
#| fig-cap: Energy Variable
#| echo: false
#| warning: false

av <- av |>
  mutate(is_antonoff= factor(is_antonoff))

av |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = energy, x = album_release_date, color = is_antonoff)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Energy") +
  facet_wrap(~ artist_name, ncol = 3)
```

In @fig-energy, a downward trend in the energy scores is observed among artists collaborating with Antonoff. However, this trend appears consistent with the overall trajectories of these artists' works, suggesting broader trends in the pop genre, as exemplified by Lana Del Rey and Taylor Swift. Sharon Von Etten, Maggie Rogers, and Mitski, display upward energy trajectories in their musical careers.

```{r, fig.width= 8, fig.height=6}
#| label: fig-danceability
#| fig-cap: Danceability Variable
#| echo: false
#| warning: false

av |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = danceability, x = album_release_date, color = is_antonoff)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Danceability") +
  facet_wrap( ~ artist_name, ncol = 3)
```

No significant trends are evident in danceability across the artists' collaborations with Antonoff, as demonstrated in (@fig-danceability) .

```{r, fig.width= 8, fig.height=6}
#| label: fig-valence
#| fig-cap: Valence Variable
#| echo: false
#| warning: false

av |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = valence, x = album_release_date, color = is_antonoff)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Valence") +
  facet_wrap( ~ artist_name, ncol = 3)
```

@fig-valence reveals a notable shift in valence scores in Lorde's discography, where her tracks display a slight upward trend and a greater range in valence scores over time.

```{r, fig.width= 8, fig.height=6}
#| label: fig-acousticness
#| fig-cap: Acousticness Variable
#| echo: false
#| warning: false

av |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = acousticness, x = album_release_date, color = is_antonoff)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Acousticness") +
  facet_wrap( ~ artist_name, ncol = 3)
```

@fig-acousticness underscores a trend toward increased acousticness in the works of Taylor Swift, Lorde, and HAIM, contrasted with declining trends in artists like Mitski, Sharon Von Etten, and Maggie Rogers.

```{r, fig.width= 8, fig.height=6}
#| label: fig-instrumentalness
#| fig-cap: Instrumentalness Variable
#| echo: false
#| warning: false

av |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = instrumentalness, x = album_release_date, color = is_antonoff)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Instrumentalness") +
  facet_wrap( ~ artist_name, ncol = 3)
```

@fig-instrumentalness exhibits no significant correlations.

```{r, fig.width= 8, fig.height=6}
#| label: fig-tempo
#| fig-cap: Tempo Variable
#| echo: false
#| warning: false

av |>
  filter(album_release_date_precision == "day") |>
  mutate(album_release_date = ymd(album_release_date)) |>
  ggplot(aes(y = tempo, x = album_release_date, color = is_antonoff)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal() +
  scale_color_manual(values = c("#808080", "#FFB6C1")) +
  labs(color = "Has 'y' in jack?", x = "Album Release Date", y = "Tempo") +
  facet_wrap( ~ artist_name, ncol = 3)
```

Finally, tempo trends, as shown in Figure @fig-tempo, are generally stable across artists, with notable exceptions in collaborations involving Lana Del Rey and Taylor Swift.

We've observed subtle distinctions in variables associated with songs produced by Jack Antonoff, as evidenced by our cumulative distribution functions and minor trends in the evolution of artists' sound characteristics, particularly energy. This study sets the stage fora) employing logistic regression to quantify the likelihood of Antonoff's involvement in a song based on these audio features, b) classifying tracks produced by a wider range of producers using a Random Forest, c) identify the most influential variables in this classification.

# Model

## Logistic Regression

The aim of the logistic regression is to determine whether we can predict Antonoff's influence in artists' sound. We select a total of 1465 tracks from the dataset from 12 artists: Lorde, Taylor Swift, St. Vincent, Lana Del Rey, The Chicks, Florence and the Machine, HAIM, Marina and the Diamonds, Maggie Rogers, Sharon Von Etten, Mitski, and Bleachers (Antonoff's own band).

The binary response variable, called `is_antonoff`, has a value of 1 if the song was produced by Antonoff, and a 0 otherwise. There are 10 predictor variables, all of which are continuous: **`danceability`**, **`energy`**, **`loudness`**, **`mode`**, **`speechiness`**, **`acousticness`**, **`instrumentalness`**, **`liveness`**, **`valence`**, and **`tempo`**.

The formula for this logistic regression can be written as:

$$
Pr(yi​=1)=logit−1(β0​+β1​×danceability+β2​×energy+...+β10​×tempo)
$$

where $\text{Pr}(y_i=1)$ is the probability of a song being produced by Antonoff, $\beta_0$ is the intercept, and $\beta_1, \ldots, \beta_{10}$ are the coefficients corresponding to the 10 predictor variables.

After some minor pre-processing using the `tidyverse` R package [@citetidyverse]. We followed @citeSilge to build a model that deals with the imbalanced classes.

We used the `tidymodels` to fit the model [@citetidymodels]. To deal with the imbalanced classes, we downsampled the majority class using the **`step_downsample()`** function from the **`themis`** package. Then we used 5-fold cross-validation with stratification by the **`is_antonoff`** variable to estimate the performance of the model.

![](images/lr_ds_orig.jpeg)

## Random Forest

Random forests (RF) build an ensemble of decision trees during the training phase, and then use majority voting for classification tasks. Each tree in the ensemble is trained on a random sample of data taken with replacement, and only a random subset of features are considered when splitting at each node. This method ensures that each tree in the ensemble is unique, avoiding overfitting and capturing non-linear relationships and more complex patterns in the data.

The training process for the RF model can be summarized as follows:

1.  For each iteration $(b = 1)$ to $( B )$:
    a.  Draw a bootstrap sample $( Z^* )$ of size $( N )$ from the training data.
    b.  Grow a random-forest tree $( T_b )$ to the bootstrapped data by recursively performing the following steps for each terminal node of the tree until the minimum node size $( n_{{min}} )$ is reached:
        i.  Select $( m )$ predictors at random from the $( p )$ predictors.
        ii. Determine the best predictor and split-point among the $( m )$.
        iii. Split the node into two daughter nodes.
2.  The ensemble of trees $( \{ T_b \}_{b=1}^B )$ constitutes the random forest model.

For a new instance $( x )$, the RF model prediction $( \hat{C}^{RF}(x) )$ is determined by the majority vote from the ensemble of trees:

$$
\hat{C}^{\text{RF}}(x) = \text{majority vote} \left\{ \hat{C}_b(x) \right\}_{b=1}^B 
$$

Where $( \hat{C}_b(x) )$ is the class prediction of the $(b)$-th tree.

The categorical feature `producer`, which contains the names of the producer for each track, was one-hot encoded to create binary columns for each producer (i.e. `is_antonoff`, `is_elworth`, etc.). The producer feature was then converted into a factor variable, and the dataset was split into training and tests sets.

The RF was trained using the @ranger package. The 'importance' parameter was set to 'impurity', which sets the model to interpret variable importance using Gini impurity.

Gini impurity is a measure of misclassification. At every split in a decision tree, the algorithm tests how well each feature splits the data. The features that split the data the most accurately, and then with the least impurity, would have the lowest impurity score. In Random Forests, a variable's importance can be calculated by looking at how much the tree nodes that use that variable reduce impurity on average.

Gini impurity was chosen on the grounds that this study is interested in not only classifying producers, but, more importantly, in understanding how the underlying data informs these decisions. By calculating the Gini impurity not only for the entire model, but specifically for each producer, we can better understand how the variables in our dataset describe the differences between producers. Without song structure, instruments, sound, or other more ephemeral components of style, these differences can be taken as a proxy for style.

# Results

## Logistic Regression

The model's accuracy score is 0.757, and the ROC AUC score is 0.819. The precision score is 0.3851, meaning that approximately 38.51% of songs predicted to be produced by Antonoff were actually produced by him. The recall of 0.7561 indicates that about 75.61% of songs produced by Antonoff were actually correctly identified in the model.

|                        | Actually Positive (1) | Actually Negative (0) |
|------------------------|-----------------------|-----------------------|
| Predicted Positive (1) | 62                    | 99                    |
| Predicted Negative (0) | 20                    | 308                   |

: Confusion Matrix for Logistic Regression Results

@tbl-importance shows variables' respective importance in determining whether a track was produced by Antonoff.

```{r}
#| label: tbl-importance
#| tbl-cap: Importance Scores of Each Variable in Determining Antonoff's Tracks
#| echo: false
#| warning: false

antonoff_vip <- read_csv(here::here("inputs/data/antonoff_vip.csv"))

antonoff_vip |>
  knitr::kable(col.names = c("Variable",
                             "Importance",
                             "Sign"))
```

```{r}
#| label: fig-importance
#| fig-cap: Importance Scores of Each Variable in Determining Antonoff's tracks
#| echo: false
#| warning: false

antonoff_vip |>
  group_by(Sign) |>
  slice_max(Importance, n = 15) |>
  ungroup() |>
  ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) +
  geom_col() +
  facet_wrap(vars(Sign), scales = "free_y") +
  labs(y = NULL) +
  theme(legend.position = "none")
```

`Danceability`, with a positive importance score of 4.142, had the highest level of importance in determining whether a song was produced or co-produced by Jack Antonoff. `Valence` was the second most important, with a negative score of 2.559. The third, `instrumentalness` correlates negatively with Antonoff tracks with a negative score of 2.309.

```{r}
#| label: tbl-mostdanceable
#| tbl-cap: Top 20 Most Danceable Antonoff-Produced Songs
#| echo: false
#| warning: false

clean_df <- read_csv(here::here("inputs/data/clean_df.csv"))

clean_df |>
  filter(producer == "antonoff") |>
  arrange(desc(danceability)) |>
  distinct(track_name, .keep_all = TRUE) |>
  select(artist_name, danceability, track_name) |>
  head(20) |>
  knitr::kable(col.names = c("Artist Name",
                             "Danceability",
                             "Track"))
```

In the 20 songs in Jack Antonoff's discography with the highest `danceability` scores, Bleachers tracks account for 35% of this list, and Taylor Swift tracks account 40% of this list (@tbl-mostdanceable). St Vincent, The Chicks, Lorde, and Florence and the Machine are also featured.

## Random Forest

| Metric                  | Value           |
|-------------------------|-----------------|
| Accuracy                | 0.6111          |
| 95% Confidence Interval | (0.579, 0.6425) |
| No Information Rate     | 0.4274          |
| P-Value (Acc \> NIR)    | \<2.2e-16       |
| Kappa                   | 0.4223          |

: Overall Performance Statistics

The accuracy of the model (0.6111) being significantly higher than the No Information Rate (0.4274) means that it is doing more than just blind guessing based on class prevalence. With a p-value below 0.05, we can confirm the model's statistical significance over a naive approach.

The model's accuracy and Kappa statistics suggest a moderate level of performance. However, there is enough demonstrated statistical significance to suggest that there are underlying patterns in the data that differentiate producers.

Correlation heatmaps can help us better understand the differences between each producer's "style". The variables with the most variance were danceability, loudness, and energy.

```{r}
#| echo: false
#| warning: false
## Range of correlation scores

library(dplyr)

table_data <- read_csv(here::here("inputs/data/table_data.csv"))

range_data <- table_data %>%
  gather(variable, value, -producer, -term) %>%
  group_by(variable) %>%
  summarize(
    min_value = min(value, na.rm = TRUE),
    max_value = max(value, na.rm = TRUE),
    range = max_value - min_value
  ) %>%
  arrange(-range)


##Variables with greatest variance##
top_vars <- range_data %>% 
  top_n(3, range) %>%
  pull(variable)

print(range_data)



```

Similar to the logistic regression, danceability was also, across all producers, the most important predictor in the classification process.

```{r, fig.width= 8, fig.height=6}
#| echo: false
#| warning: false

df_importance<- read_csv(here::here("inputs/data/df_importance.csv"))

df_importance_long <- df_importance %>%
  gather(variable, importance, -producer)

ggplot(df_importance_long, aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity") +
  facet_wrap(~producer, scales = "fixed") + 
  coord_flip() +
  labs(title = "Variable Importance by Producer", 
       x = "Variable", 
       y = "Mean Decrease Gini") +
  theme_minimal()


```

Relative to other producers, Antonoff's danceability has the least positive correlation with valence (0.311), the most negative correlation with acousticness (-0.0733), and significant positive correlations with speechiness (0.336), loudness (0.366), and energy (0.274).

```{r}
#| label: fig-dancevariance
#| echo: false
#| warning: false
# Danceability Plot

selected_data <- read_csv(here::here("inputs/data/selected_data.csv"))

ggplot(selected_data, aes(x=term, y=danceability, fill=producer)) +
  geom_bar(stat="identity", position="dodge") +
  labs(title="Danceability across Producers", y="Danceability Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Discussion

Our logistic regression model was not able to strongly predict whether or not a song was produced by Jack Antonoff using the Spotify API's audio features data. These data are limited, providing static metrics that stand in for entire tracks, which may not be detailed enough for intra-genre classification. However, when classifying the tracks, `danceability` was the strongest predictor.

Our Random Forest results, which were moderately strong at classifying tracks, indicate that Antonoff's music, characterized by its `danceability` score, exhibits a significant positive correlation with factors such as `speechiness`, `loudness`, and `energy`, while showing a negative correlation with `acousticness`, and a less pronounced positive correlation with `valence`, compared to other music producers. This profile---marked by speechiness, loudness, energy, synthetic textures, and a blend of positivity with a touch of nostalgia---aligns with both detailed auditory analysis and critical assessments of Antonoff's sound.

Antonoff's tracks have been described as anthemic, with a distinct vocal treatment that makes it sound, as Gamman (2022) observes, "the way it would sound to the person who's singing them".

> In the treble mix of the song, like in the very upper end, sort of above where the vocals are -- he often really crushes that down. It's lower in volume than anything else, which is sort of a weird effect. When you hear someone speaking, you hear a lot of that, like, noise in their voice. When you yourself are speaking, you hear less of it, right? You hear more of your own voice bouncing around in your head. And so it sort of creates this effect of -- **like he mixes his vocals the way it would sound to the person who's singing , them. Which is sort of strange.** He often has no vocals on that upper end, which is very unusual, and then often there are random little bits of noise happening up there which is the sort of thing her likes to do.

Perhaps, without further documentation on the `speechiness` feature's construction, clearing the noise and isolating the middle range of the vocals may make for stronger overall `speechiness` score in Antonoff's tracks. Antonoff also tends to collaborate with singer-songwriter artists, whose music is more lyric-oriented than dance hit pop.

Even Antonoff-produced songs with the highest `danceability` scores aren't exactly dance hits, compared to production styles like that of Max Martin.

For instance, the highest scoring song in danceability*, I Think He Knows* by Taylor Swift, seems danceable, yet the next highest-scoring songs -- Taylor Swift's *Vigilante Shit* or Florence and the Machine's *Heaven is Here* -- don't resonate as dance tracks. However, they aren't understated, ambient tracks either -- the exhibit some sort of compelling quality, like they're coercing your body to do something. Like Gamman's description of Antonoff's vocal design sounding like how they would sound to the person who's singing them, these tracks seem like they're trying to get inside of your skin. But they don't have the rhythm of a dance track.

*I* *Wanna Get Better* by Bleachers typifies the Antonoff style as identified by our model. This song is more anthemic, resonating with a sort of neo-Springsteen aesthetic that Antonoff is often characterized (or admitting) to striving for, than danceable. It also does sound like it's trying to demand your attention, and the speechiness, loudness, and energy which are distinctly correlated with danceability in Antonoff's RF results, are all apparent. The redemptive, broken-but-getting-better tone can also be heard in the lyrics, but it still doesn't have the kind of rhythm for dancing -- which maps onto how Antonoff's "valence" score isn't as a high as other pop artists in @fig-dancevariance.

*Cornelia Street* by Taylor Swift further exemplifies this near-danceable category. While melodically catchy, the lyrics themselves are wistful and longing. The vocal treatment, as described by Gamman, is there -- Swift's vocals sound magnified, carrying the bridge of the song.

What does the danceability variable signify in Antonoff's music? Our findings suggest that his most "danceable" songs are better understood through the lens of correlated features, even though danceability emerges as the primary predictor. This crowd-sourced metric, in conjunction with machine-listening-derived features, offers a unique insight into an artist's style, capturing an intuitive understanding of a song's tone and energy.

We spoke with Glen McDonald, a Principal Engineer at Spotify who worked at The Echo Nest, a music intelligence start-up that was acquired by Spotify in 2014. Macdonald revealed that `valence` and `danceability` were created by giving tracks to college interns and asking them to tag whether a song was positive or gloomy, or danceable or un-danceable. Variables like `energy` or `instrumentalness`, on the other hand, were determined primarily through machine listening techniques, with human subjectivity being applied to fine-tune the features[^1]:

[^1]: A notable example of this being that bluegrass songs were given very high `speechiness` ratings. Because there were no banjos in the training data, the instruments were registered as human speech. The engineers had to go back and add more songs with banjos to the training data.

> "You could imagine writing a formula for energy that combines loudness and tempo and degree of harmonic variation or something. So that feature was \[machine learning\], but that one's more like a human helping a machine figure out a formula. Whereas valence is teaching the machine to try to reproduce a purely human thing. The same with danceability. I mean, danceability is whether a human can dance to it. The machine's not gonna dance, so the machine can't have any opinion on that. The computer could have an opinion on energy. And the computer can definitely have an opinion on loudness. So there's a spectrum from, loudness as purely analytical, and then energy is a little like loudness, with a little more subjectivity. And then danceability and valence are purely subjective."

Glen confirmed that this process didn't account for lyrics, which made valence a particularly difficult variable to build. It could, in theory, pick up on aspects of vocal performance, but it didn't process anything about languages or words or the meanings of songs. Take an upbeat, happy-sounding Elliot Smith song with devastating lyrics: the machine might register it as happy, where human listeners understand that it's sad. Furthermore, two humans might even disagree on the song's valence: "Plus, we have the confounding factor of like, a song that seems happy today could be sad tomorrow because the singer was killed in a plane crash. The song didn't change, but our world changed and our reactions changed."

Macdonald suggests combining energy and valence to create quadrant, which usually works "fairly well" to describe music (see @fig-quadvalence). High energy and high valence could be generally happy, cheerful, upbeat. Low energy, low valence could be sad or downbeat. High energy, low valence is sort of angry. Low energy, high valence is serene or calming.

```{r}
#| label: fig-quadvalence
#| fig-cap: Valence and Energy Quadrant
#| echo: false
#| warning: false
av <- read.csv(here::here("inputs/data/antonoff_viz.csv"))

av$is_antonoff <- ifelse(av$producer == "antonoff", 1, 0)
av$is_antonoff <- as.factor(av$is_antonoff)

ggplot(av, aes(x = energy, y = valence)) +
  geom_point(aes(color = as.factor(is_antonoff)), alpha = 0.3) +
  geom_vline(xintercept = mean(av$energy), color = "gray") +
  geom_hline(yintercept = mean(av$valence), color = "gray") +
  theme_minimal()
```

Our analysis reveals that the amalgamation of crowdsourced metrics (danceability) and machine-listening variables offers a nuanced approximation of musical style. Interestingly, the subjective nature of crowdsourced metrics, which might appear rudimentary, plays a crucial role in shaping a more intuitive and embodied understanding of music. This synergy between human subjectivity and algorithmic precision not only mirrors the multifaceted nature of musical perception, but also enriches our model's capacity to capture the essence of an artist's style.

\newpage

# References
